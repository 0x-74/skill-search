import streamlit as st
import os
import time
import pandas as pd
from typing import Literal, Union
import dspy
from dotenv import load_dotenv
from visualizations import (
    show_live_analysis_in_placeholder, 
    show_final_analysis_results
)

def show_job_parser_page():
    st.subheader("ğŸ§  AI Job Parser & Analyzer")
    st.write("Parse and analyze scraped job listings using advanced AI models")
    
    # Initialize session state
    init_session_state()
    
    # Configuration section
    with st.expander("âš™ï¸ LLM Configuration", expanded=True):
        show_llm_config()
    
    # File upload/selection section
    st.markdown("---")
    st.subheader("ğŸ“ Job File Selection")
    
    # Tab for different input methods
    tab1, tab2 = st.tabs(["ğŸ“¤ Upload File", "ğŸ“‚ Select Local File"])
    
    with tab1:
        uploaded_file = st.file_uploader(
            "Upload job listings file",
            type=['txt'],
            help="Upload the .txt file generated by the scraper"
        )
        
        if uploaded_file:
            # Save uploaded file temporarily
            temp_path = f"temp_{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getvalue())
            st.session_state.selected_file_path = temp_path
            st.success(f"âœ… File uploaded: {uploaded_file.name}")
    
    with tab2:
        # List local .txt files
        txt_files = [f for f in os.listdir('.') if f.endswith('.txt')]
        
        if txt_files:
            selected_file = st.selectbox(
                "Select a job file",
                options=txt_files,
                help="Choose from locally available job files"
            )
            
            if selected_file:
                st.session_state.selected_file_path = selected_file
                
                # Show file preview
                with open(selected_file, 'r', encoding='utf-8') as f:
                    preview = f.read()[:1000]
                
                st.text_area("File Preview", preview + "...", height=150, disabled=True)
                st.info(f"ğŸ“Š File size: {len(preview)} characters (showing first 1000)")
        else:
            st.info("No .txt files found in the current directory")
    
    # Parsing configuration
    if 'selected_file_path' in st.session_state and st.session_state.selected_file_path:
        st.markdown("---")
        st.subheader("ğŸ”§ Parsing Configuration")
        
        # Advanced configuration in expander
        with st.expander("âš™ï¸ Advanced Settings", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                separator = st.text_input(
                    "Job Separator",
                    value="-" * 80,
                    help="Text that separates individual job listings"
                )
                
                sleep_time = st.number_input(
                    "Sleep Between Jobs (seconds)",
                    min_value=0,
                    max_value=300,
                    value=6,
                    help="Delay between parsing jobs to avoid rate limits"
                )
                
                show_individual_logs = st.checkbox(
                    "Show Individual Job Logs",
                    value=False,
                    help="Display detailed logs for each job (may slow down UI)"
                )
            
            with col2:
                update_frequency = st.number_input(
                    "Update Charts Every N Jobs",
                    min_value=1,
                    max_value=50,
                    value=5,
                    help="How often to update visualizations during parsing"
                )
                
                max_preview_jobs = st.number_input(
                    "Max Jobs in Preview Table",
                    min_value=3,
                    max_value=20,
                    value=5,
                    help="Number of latest jobs to show in preview table"
                )
                
                enable_live_charts = st.checkbox(
                    "Enable Live Charts",
                    value=True,
                    help="Show real-time charts during parsing"
                )
        
        # Chart configuration
        with st.expander("ğŸ“Š Chart Configuration", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                chart_height = st.slider(
                    "Chart Height (px)",
                    min_value=200,
                    max_value=800,
                    value=400,
                    help="Height of the visualization charts"
                )
                
                max_categories_shown = st.slider(
                    "Max Categories in Charts",
                    min_value=5,
                    max_value=25,
                    value=10,
                    help="Maximum number of categories to show in bar/pie charts"
                )
            
            with col2:
                color_scheme = st.selectbox(
                    "Color Scheme",
                    ["viridis", "plasma", "inferno", "magma", "cividis", "turbo"],
                    index=0,
                    help="Color scheme for the charts"
                )
                
                show_data_labels = st.checkbox(
                    "Show Data Labels",
                    value=True,
                    help="Show values on charts"
                )
        
        # Store config in session state
        st.session_state.parsing_config = {
            'separator': separator,
            'sleep_time': sleep_time,
            'update_frequency': update_frequency,
            'max_preview_jobs': max_preview_jobs,
            'show_individual_logs': show_individual_logs,
            'enable_live_charts': enable_live_charts,
            'chart_height': chart_height,
            'max_categories_shown': max_categories_shown,
            'color_scheme': color_scheme,
            'show_data_labels': show_data_labels
        }
        
        # Parsing execution
        if st.button("ğŸš€ Start Parsing Jobs", type="primary", use_container_width=True):
            reset_parsing_state()
            st.session_state.parsing_in_progress = True
            st.rerun()
    
    # Show parsing interface if in progress
    if st.session_state.get('parsing_in_progress', False):
        show_parsing_interface()
    
    # Show final results if parsing is complete and we have data
    if (not st.session_state.get('parsing_in_progress', False) and 
        st.session_state.get('parsing_state') and 
        st.session_state.parsing_state.get('parsed_jobs')):
        
        st.markdown("---")
        if st.button("ğŸ“Š View Final Analysis", type="primary", use_container_width=True):
            st.session_state.show_final_analysis = True
            st.rerun()
        
        if st.session_state.get('show_final_analysis', False):
            show_final_analysis_results()

def init_session_state():
    """Initialize session state variables"""
    if 'llm_configured' not in st.session_state:
        st.session_state.llm_configured = False
    
    if 'parsing_in_progress' not in st.session_state:
        st.session_state.parsing_in_progress = False
    
    if 'show_final_analysis' not in st.session_state:
        st.session_state.show_final_analysis = False

def reset_parsing_state():
    """Reset parsing state for a new parsing session"""
    st.session_state.parsing_state = {
        'current_job': 0,
        'total_jobs': 0,
        'parsed_jobs': [],
        'failed_jobs': [],
        'parsing_started': False,
        'parsing_complete': False,
        'last_update': 0,
        'start_time': None,
        'update_counter': 0
    }
    st.session_state.show_final_analysis = False

def show_llm_config():
    """Configure LLM settings"""
    
    # Model selection
    model_type = st.radio(
        "Select LLM Provider",
        ["Groq (Cloud)", "Ollama (Local)"],
        horizontal=True
    )
    
    col1, col2 = st.columns(2)
    
    if model_type == "Groq (Cloud)":
        with col1:
            api_key = st.text_input(
                "Groq API Key",
                type="password",
                value=os.getenv("GROQ_API_KEY", ""),
                help="Your Groq API key"
            )
        
        with col2:
            model_name = st.selectbox(
                "Model",
                ["groq/llama-3.3-70b-versatile", "groq/llama-3.1-70b-versatile", "groq/mixtral-8x7b-32768"],
                help="Select Groq model"
            )
        
        if st.button("ğŸ”— Configure Groq"):
            if not api_key.strip():
                st.error("âŒ Missing API key! Please provide a valid Groq API key.")
            else:
                try:
                    lm = dspy.LM(model_name, api_key=api_key)
                    dspy.configure(lm=lm)
                    st.session_state.llm_configured = True
                    st.session_state.current_model = model_name
                    st.success("âœ… Groq LLM configured successfully!")
                except Exception as e:
                    st.error(f"âŒ Failed to configure Groq: {e}")

    else:  # Ollama Local
        with col1:
            api_base = st.text_input(
                "Ollama API Base",
                value="http://localhost:11434",
                help="Ollama server URL"
            )
        
        with col2:
            model_name = st.text_input(
                "Model Name",
                value="gemma3:27b-it-qat",
                help="Ollama model name"
            )
        
        if st.button("ğŸ”— Configure Ollama"):
            try:
                lm = dspy.LM(
                    f"ollama_chat/{model_name}",
                    api_base=api_base,
                    api_key="",
                    model_type="chat",
                )
                dspy.configure(lm=lm)
                st.session_state.llm_configured = True
                st.session_state.current_model = f"ollama/{model_name}"
                st.success("âœ… Ollama LLM configured successfully!")
            except Exception as e:
                st.error(f"âŒ Failed to configure Ollama: {e}")
    
    # Show current configuration
    if st.session_state.get('llm_configured', False):
        st.info(f"ğŸ¤– Current Model: {st.session_state.get('current_model', 'Unknown')}")

def show_parsing_interface():
    """Show the parsing progress interface"""
    
    st.markdown("---")
    st.subheader("ğŸ”„ Parsing in Progress")
    
    if not st.session_state.get('llm_configured', False):
        st.error("âŒ Please configure an LLM model first!")
        st.session_state.parsing_in_progress = False
        return
    
    config = st.session_state.get('parsing_config', {})
    
    # Initialize parsing state if needed
    if 'parsing_state' not in st.session_state or st.session_state.parsing_state is None:
        reset_parsing_state()
    
    # Create FIXED containers that will be reused (prevent stacking)
    metrics_container = st.container()
    progress_container = st.container()
    controls_container = st.container()
    
    # Logs container (only if enabled)
    if config.get('show_individual_logs', False):
        logs_container = st.container()
    else:
        logs_container = None
    
    # FIXED Live visualization container
    if config.get('enable_live_charts', True):
        st.markdown("---")
        st.subheader("ğŸ“Š Live Analysis")
        viz_placeholder = st.empty()
    else:
        viz_placeholder = None
    
    # Populate containers
    with metrics_container:
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            current_job_metric = st.empty()
        with col2:
            success_rate_metric = st.empty()
        with col3:
            time_remaining_metric = st.empty()
        with col4:
            jobs_per_min_metric = st.empty()
    
    with progress_container:
        progress_bar = st.progress(0)
        status_text = st.empty()
    
    with controls_container:
        col1, col2 = st.columns(2)
        
        with col1:
            if not st.session_state.parsing_state['parsing_started']:
                if st.button("â–¶ï¸ Start Parsing", type="primary"):
                    st.session_state.parsing_state['parsing_started'] = True
                    st.session_state.parsing_state['start_time'] = time.time()
                    st.rerun()
            else:
                st.button("â¸ï¸ Parsing...", disabled=True)
        
        with col2:
            if st.button("â¹ï¸ Stop Parsing"):
                st.session_state.parsing_in_progress = False
                st.session_state.parsing_state['parsing_started'] = False
                st.session_state.parsing_state['parsing_complete'] = True
                st.rerun()
    
    # Execute parsing if started
    if st.session_state.parsing_state['parsing_started']:
        execute_job_parsing(
            current_job_metric, success_rate_metric, time_remaining_metric, jobs_per_min_metric,
            progress_bar, status_text, logs_container, viz_placeholder
        )

def execute_job_parsing(current_metric, success_metric, time_metric, rate_metric, progress_bar, status_text, logs_container, viz_placeholder):
    """Execute the actual job parsing process"""
    
    config = st.session_state.get('parsing_config', {})
    
    try:
        # Initialize DSPy components
        load_dotenv()
        
        class JobDetails(dspy.Signature):
            """Extract detailed job information from a listing."""
            job_text: str = dspy.InputField()
            company: str = dspy.OutputField(desc="Name of the company")
            job_title: str = dspy.OutputField(desc="standardized format: seniority (junior, senior), title (full-stack, web, ml), role (engineer, intern)")
            location: str = dspy.OutputField(desc="Job location")
            domain: str = dspy.OutputField(desc="Domain of work: AI, web, development, full-stack")
            domain_specific_skills: str = dspy.OutputField(desc="Technical skills relevant to the domain (e.g., opencv, pandas, numpy, tensorflow)")
            work_model: str = dspy.OutputField(desc="Work model (e.g., Hybrid, Full-time)")
            min_experience: int = dspy.OutputField(desc="Minimum experience in years")
            max_experience: int = dspy.OutputField(desc="Maximum experience in years")
            number_of_employeees: str = dspy.OutputField(desc="Number of employees in the company")
            weeks_since_posting: int = dspy.OutputField(desc="Weeks passed since posting date")
            min_salary: Union[None, int] = dspy.OutputField(desc="Minimum pay range")
            max_salary: Union[None, int] = dspy.OutputField(desc="Maximum pay range")
            no_of_applicants: int = dspy.OutputField(desc="Number of people that clicked apply")
            company_type: str = dspy.OutputField(desc="Type, size, funding stage")
            key_responsibilities: str = dspy.OutputField(desc="Key responsibilities / tasks")
            technical_requirements: str = dspy.OutputField(desc="Required technical skills / technologies")
            education: Literal["Bachelors", "Masters", "Phd"] = dspy.OutputField(desc="Minimum required educational background")
            benefits_culture: str = dspy.OutputField(desc="Company culture, benefits, perks")
            unique_aspects: str = dspy.OutputField(desc="Unique aspects of the role or company")
            application_link: Union[None, str] = dspy.OutputField(desc="How to apply / URL")
            applicant_insights: str = dspy.OutputField(desc="Insights on typical applicants / experience levels")
        
        job_parser = dspy.ChainOfThought(JobDetails)
        
        # Read and split file
        with open(st.session_state.selected_file_path, "r", encoding="utf-8") as f:
            chunks = [c.strip() for c in f.read().split(config.get('separator', '-' * 80)) if c.strip()]
        
        st.session_state.parsing_state['total_jobs'] = len(chunks)
        
        # Process each job
        for idx, chunk in enumerate(chunks):
            if not st.session_state.parsing_state.get('parsing_started', False):
                break
                
            st.session_state.parsing_state['current_job'] = idx + 1
            
            # Update status text
            status_text.text(f"Processing job {idx + 1} of {len(chunks)}...")
            
            # Update metrics
            current_metric.metric("Current Job", f"{idx + 1} / {len(chunks)}")
            
            success_count = len(st.session_state.parsing_state['parsed_jobs'])
            total_processed = success_count + len(st.session_state.parsing_state['failed_jobs'])
            success_rate = (success_count / max(total_processed, 1)) * 100
            success_metric.metric("Success Rate", f"{success_rate:.1f}%")
            
            # Calculate time remaining and rate
            if st.session_state.parsing_state['start_time']:
                elapsed_time = time.time() - st.session_state.parsing_state['start_time']
                if idx > 0:
                    avg_time_per_job = elapsed_time / idx
                    remaining_jobs = len(chunks) - idx
                    time_remaining = remaining_jobs * avg_time_per_job
                    time_metric.metric("Est. Time Remaining", f"{time_remaining/60:.1f} min")
                    
                    jobs_per_minute = idx / (elapsed_time / 60) if elapsed_time > 0 else 0
                    rate_metric.metric("Jobs/Minute", f"{jobs_per_minute:.1f}")
            
            # Update progress
            progress = idx / len(chunks)
            progress_bar.progress(progress)
            
            # Parse job
            try:
                if logs_container and config.get('show_individual_logs', False):
                    with logs_container:
                        with st.status(f"ğŸ”„ Processing job {idx + 1}...", expanded=False) as status:
                            st.write(f"Job preview: {chunk[:100]}...")
                
                result = job_parser(job_text=chunk)
                job_dict = result.toDict()
                job_dict['_job_index'] = idx
                job_dict['_parsed_at'] = time.time()
                st.session_state.parsing_state['parsed_jobs'].append(job_dict)
                
                if logs_container and config.get('show_individual_logs', False):
                    with logs_container:
                        st.success(f"âœ… Job {idx + 1}: {job_dict.get('company', 'Unknown')} - {job_dict.get('job_title', 'Unknown')}")
                
            except Exception as e:
                st.session_state.parsing_state['failed_jobs'].append({
                    'index': idx,
                    'error': str(e),
                    'chunk_preview': chunk[:100],
                    'failed_at': time.time()
                })
                
                if logs_container and config.get('show_individual_logs', False):
                    with logs_container:
                        st.error(f"âŒ Failed job {idx + 1}: {str(e)[:100]}")
            
            # Update visualizations at specified frequency
            jobs_parsed = len(st.session_state.parsing_state['parsed_jobs'])
            should_update = (
                config.get('enable_live_charts', True) and
                jobs_parsed > 0 and 
                (jobs_parsed % config.get('update_frequency', 5) == 0 or idx == len(chunks) - 1)
            )
            
            if should_update and viz_placeholder:
                st.session_state.parsing_state['update_counter'] += 1
                show_live_analysis_in_placeholder(viz_placeholder)
            
            # Sleep between jobs (if not the last job)
            sleep_time = config.get('sleep_time', 0)
            if sleep_time > 0 and idx < len(chunks) - 1:
                status_text.text(f"â±ï¸ Waiting {sleep_time} seconds before next job...")
                time.sleep(sleep_time)
        
        # Completion
        progress_bar.progress(1.0)
        status_text.text("âœ… Parsing completed!")
        st.session_state.parsing_state['parsing_complete'] = True
        
        if logs_container:
            with logs_container:
                st.balloons()
                st.success(f"ğŸ‰ Parsing completed! {len(st.session_state.parsing_state['parsed_jobs'])} jobs parsed successfully")
                
                if st.session_state.parsing_state['failed_jobs']:
                    st.warning(f"âš ï¸ {len(st.session_state.parsing_state['failed_jobs'])} jobs failed to parse")
        
        # Final visualization update
        if viz_placeholder and config.get('enable_live_charts', True):
            st.session_state.parsing_state['update_counter'] += 1
            show_live_analysis_in_placeholder(viz_placeholder)
        
        # Save results
        if st.session_state.parsing_state['parsed_jobs']:
            df = pd.DataFrame(st.session_state.parsing_state['parsed_jobs'])
            output_file = f"parsed_jobs_{int(time.time())}.csv"
            df.to_csv(output_file, index=False)
            
            status_text.text(f"ğŸ’¾ Results saved to {output_file}")
            
            # Download button
            if logs_container:
                with logs_container:
                    csv_data = df.to_csv(index=False)
                    st.download_button(
                        "ğŸ“¥ Download Parsed Jobs CSV",
                        data=csv_data,
                        file_name=output_file,
                        mime="text/csv",
                        type="primary"
                    )
    
    except Exception as e:
        status_text.text(f"âŒ Parsing failed: {e}")
        if logs_container:
            with logs_container:
                st.error(f"âŒ Parsing failed: {e}")
    
    finally:
        st.session_state.parsing_state['parsing_started'] = False
        st.session_state.parsing_in_progress = False
        st.session_state.parsing_state['parsing_complete'] = True